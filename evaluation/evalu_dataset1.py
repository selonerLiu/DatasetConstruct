import datetime
import json
import os
import re
import time
from pathlib import Path
import yaml
from langchain_openai import ChatOpenAI


# 1.ä»æ–‡ä»¶ä¸­è¯»å–è¯•é¢˜åºåˆ—
def load_questions(input_dir=""):
    """
    åŠ è½½æ•°æ®é›†ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥åŠ è½½ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„å…¨éƒ¨jsonï¼Œå°±ä¸ç”¨å‰é¢é‚£ä¸ªåˆå¹¶çš„æ“ä½œäº†ã€‚
    :param input_dir: è¾“å…¥æ–‡ä»¶å¤¹
    :return: è¿”å›é—®é¢˜åˆ—è¡¨
    """
    questions = []
    for filename in os.listdir(input_dir):
        if filename.endswith(".json"):
            file_path = os.path.join(input_dir, filename)
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                questions.extend(data)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(questions)} é“é¢˜ç›®")
    print(questions[0])
    return questions


# 2.è°ƒç”¨å¤§æ¨¡å‹å¯¹å•ä¸ªé—®é¢˜ä½œç­”
def get_model_answer(question, llm):
    """
    è°ƒç”¨å¤§æ¨¡å‹å¯¹å•ä¸ªé—®é¢˜ä½œç­”
    :param question: å•ä¸ªé—®é¢˜
    :return: å¤§æ¨¡å‹ä½œç­”ç»“æœï¼Œåªè¿”å›ä¸€ä¸ªç»“æœ
    """
    prompt = f"""
            Please select the correct answer based on the following question and options (return only the letter of the option A-D, no explanation):
            questionï¼š{question["question"]}
            optionsï¼š
            A. {question["options"]['A']}
            B. {question["options"]['B']}
            C. {question["options"]['C']}
            D. {question["options"]['D']}
            """

    try:

        answer = llm.invoke(prompt).content

        if answer in ["A", "B", "C", "D", "a", "b", "c", "d"]:
            print(f"âœ… æ¨¡å‹è¿”å›æœ‰æ•ˆç­”æ¡ˆï¼š{answer}")
            return answer
        else:
            print(f"âš ï¸ æ¨¡å‹è¿”å›æ— æ•ˆç­”æ¡ˆï¼š{answer}")  # è¡¨ç¤ºæ— æ•ˆç­”æ¡ˆ
            return answer
    except Exception as e:
        print(f"âŒ è°ƒç”¨æ¨¡å‹å¤±è´¥ï¼š{e}")
        return f"âŒ è°ƒç”¨æ¨¡å‹å¤±è´¥ï¼š{e}"  # è¡¨ç¤ºé”™è¯¯


# 3.å…¨éƒ¨é—®é¢˜ä½œç­”ä¸ç»“æœå­˜å‚¨
def evaluate_dataset(all_questions, llmpara_file):
    """
    è°ƒç”¨å¤§æ¨¡å‹å¯¹å…¨éƒ¨é—®é¢˜è¿›è¡Œä½œç­”ï¼Œå¹¶å°†ç»“æœå’Œç›¸å…³çš„é‡è¦ä¿¡æ¯å­˜å‚¨åˆ°æ–‡ä»¶ä¸­
    :param all_questions:æ•°æ®é›†çš„æ‰€æœ‰é¢˜ç›®
    :param llmpara_file:è°ƒç”¨çš„å„ä¸ªå¤§æ¨¡å‹çš„å‚æ•°ä¿¡æ¯
    :return:è¿”å›å­—å…¸å‹çš„ç»“æœ
    """

    results_llms = []  # ç”¨ä¸€ä¸ªåˆ—è¡¨è¡¨ç¤ºæ‰€æœ‰å¤§æ¨¡å‹åšé¢˜çš„ç»“æœä¿¡æ¯ï¼Œåˆ—è¡¨æ¯ä¸€é¡¹çš„å†…å®¹æ˜¯ä¸€ä¸ªresultsåˆ—è¡¨ã€‚

    with open(llmpara_file, "r", encoding="utf-8") as f:
        llmparameter_list = json.load(f)
    for item in llmparameter_list:
        print(f"ğŸ“Šæ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹{item['refer_model_name']}å¯¹è¯¥æ•°æ®é›†åšæµ‹è¯•è¯„ä¼°")
        llm_num = item["num"]
        llm = ChatOpenAI(model_name=item["refer_model_name"], openai_api_base=item["refer_api_base"],
                         openai_api_key=item["refer_api_key"])
        start_time = time.time()
        results = []  # ç”¨ä¸€ä¸ªåˆ—è¡¨å­˜å‚¨æŸä¸ªå¤§æ¨¡å‹äº§ç”Ÿçš„ç»“æœï¼Œåˆ—è¡¨å†…å®¹åŒ…æ‹¬é’ˆå¯¹å„é¢˜åšçš„ç»“æœï¼Œå’Œæ€»ç»“ç»“æœã€‚
        i = 0
        for i, single_question in enumerate(all_questions):
            # if i<=1004: # é’ˆå¯¹æœ‰æ—¶æ‰§è¡Œå‡ºé”™çš„æƒ…å†µï¼Œä»æŸä¸€ä¸ªåºå·å¾€åå†ç»§ç»­æ‰§è¡Œã€‚
            #     continue
            # else:
            print(f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i + 1} é¢˜ï¼š{single_question['id']}")
            llm_result = get_model_answer(single_question, llm)
            correct_answer = single_question["answer"]
            results.append({
                "id": i,
                "topic": single_question["id"],
                "question": single_question["question"],
                "correct_answer": single_question["answer"],
                "model_answer": llm_result,
                "is_correct": llm_result == correct_answer
            })
            # time.sleep(20)
        end_time = time.time()
        time_span = end_time - start_time
        correct_count = sum(1 for r in results if r["is_correct"])
        total = len(results)
        accuracy = correct_count / total * 100 if total > 0 else 0
        summary_data = {
            "start_time": start_time,
            "end_time": end_time,
            "time_span": time_span,
            "model_name": llm_num,
            "correct_count": correct_count,
            "total": total,
            "accuracy": accuracy,
            "llm_num": llm_num
        }
        combined_data = results + [summary_data]
        print(f"\nğŸ”„ æ€»ç­”é¢˜æ•°ï¼š{total}")
        print(f"âœ… æ­£ç¡®æ•°ï¼š{correct_count}")
        print(f"ğŸ¯ æ­£ç¡®ç‡ï¼š{accuracy:.2f}%")
        print("ğŸ’å°†è¯¦ç»†ç»“æœå­˜è‡³æŒ‡å®šæ–‡ä»¶......................................\n\n")
        write2file(content=combined_data, path_dir="./results/dataset1/",
                   file_name="eval_results_" + combined_data[-1]["llm_num"], file_type="json")
    results_llms.append(combined_data)
    return results_llms


def write2file(content, path_dir="", file_name="", file_type=""):
    results_dir = path_dir
    Path(results_dir).mkdir(parents=True, exist_ok=True)
    results_name = file_name
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    if file_type == "json":
        with open(f"{results_dir}/{results_name}.{file_type}", "w", encoding="utf-8") as f:
            f.write(f"// å†™å…¥æ—¶é—´: {timestamp}\n\n")
            f.write(json.dumps(content, ensure_ascii=False, indent=4))
    elif file_type == "txt":
        with open(f"{results_dir}/{results_name}.{file_type}", "w", encoding="utf-8") as f:
            f.write(f"[{timestamp}]\n\n{content}")
    elif file_type == "yaml":
        with open(f"{results_dir}/{results_name}.{file_type}", "w", encoding="utf-8") as f:
            f.write(f"// å†™å…¥æ—¶é—´: {timestamp}\n\n")
            yaml.dump(content, f, allow_unicode=True)


def run(questions_dir="../dataset1_rag_generator/output/test_set/", directory="./results/dataset1/",
        prefix_filename="eval_results_", extension="json"):
    """
    ç»¼åˆå„å‡½æ•°ï¼Œè·å–æœ€ç»ˆç»“æœï¼Œå°†ç»“æœå­˜åˆ°æ–‡ä»¶ä¸­
    """
    questions = load_questions(questions_dir)
    eval_results = evaluate_dataset(questions, "../dataset1_rag_generator/config/refer_llms.json")
    for item in eval_results:
        try:
            write2file(content=item, path_dir=str(directory), file_name=prefix_filename + item[-1]["llm_num"],
                       file_type=extension)
        except Exception as e:
            print(f"å­˜å‚¨txtæ–‡ä»¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

if __name__ == "__main__":
    # # è¯•è¯•æ•ˆæœï¼Œç”¨ä¸€ä¸ªå¤§æ¨¡å‹æµ‹è¯•å½“å‰æ•°æ®åœ¨å½“å‰æ¨¡å‹ä¸‹çš„è¿è¡Œæƒ…å†µ
    # questions = load_questions("../dataset1_rag_generator/output/test_set/")
    # # è°ƒç”¨evaluate_datasetå‡½æ•°ï¼Œå°†ç»“æœå­˜åˆ°æŒ‡å®šæ–‡ä»¶ä¸­ï¼ŒæŒ‡å®šçš„æ–‡ä»¶ç›´æ¥è°ƒç”¨write2fileåœ¨å‡½æ•°ä½“ä¸­å†™æ­»äº†ã€‚
    # results = evaluate_dataset(questions, "../dataset1_rag_generator/config/refer_llms.json")
    
    # æ­£å¼è¿è¡Œï¼Œåœ¨é—®ç­”æ•°æ®é›†ä¸­è¯„ä¼°å„å¤§æ¨¡å‹çš„æ­£ç¡®ç‡
    questions = load_questions("../dataset1_rag_generator/output/qa_set/")
    results = evaluate_dataset(questions, "../dataset1_rag_generator/config/refer_llms.json")




# def llm_answer_compare(response, benchmark):
#     """
#     ç”¨äºè¯„åˆ¤å¤§æ¨¡å‹è¾“å‡ºç»“æœæ­£ç¡®ä¸å¦ï¼Œä¸»è¦é’ˆå¯¹å¤§æ¨¡å‹æœ‰æ—¶å€™è¾“å‡ºçš„æ˜¯æ­£ç¡®çš„ä½†æ˜¯å¯¹æ¯”æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´ä¸æ­£ç¡®çš„æƒ…å†µ
#     :param response: å¤§æ¨¡å‹çš„ç›´æ¥è¾“å‡º
#     :param benchmark: æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆ
#     :return:True æˆ– False
#     """
#     stred_benchmark = ""
#     if isinstance(benchmark, list):
#         stred_benchmark = "".join(benchmark)
#         stred_benchmark = stred_benchmark.lower()
#     elif isinstance(benchmark, str):
#         stred_benchmark = benchmark.lower()
#
#     if sorted(stred_benchmark) == sorted(response):
#         return True
#     else:
#         return False


# def recycle4wrong_results():
#     """
#     å¤„ç†å¤§æ¨¡å‹è¿”å›ç»“æœä¸ç¬¦åˆé¢„æœŸçš„æƒ…å†µï¼Œå¯¹äºä¸€äº›æœ¬èº«ç­”æ¡ˆæ­£ç¡®ä½†æ˜¯å¤šäº†ä¸€äº›å†—ä½™è¡¨è¾¾è‡´ä½¿ç¨‹åºå°†å…¶åˆ¤æ–­ä¸ºè¯¯çš„æƒ…å†µï¼Œé‡æ–°è°ƒç”¨å¤§æ¨¡å‹ç›´åˆ°è¿”å›æ­£ç¡®æ ¼å¼çš„æ•°æ®ã€‚
#     :return:
#     """
#         retry = True
#         while retry:
#             response = evaluate_llm.execute_prompt(prompt)
#             print(f"é’ˆå¯¹\'{question}\'é—®é¢˜ç»™å‡ºçš„ç­”æ¡ˆæ˜¯ï¼š\n" + response)
#             # åˆ¤æ–­è¿”å›ç»“æœæ˜¯å¦ç¬¦åˆé€‰é¡¹æ ¼å¼è¦æ±‚ï¼Œå¹¶å°†å…¶å¤„ç†æˆä¸€ä¸ªçº¯å­—ç¬¦ä¸²ï¼ˆç­”æ¡ˆå¯ä»¥æ˜¯é”™çš„ï¼Œä½†æ ¼å¼ä¸€å®šè¦æ˜¯å¯¹çš„ï¼‰
#             formated_response = format_llm_response(response)
#             # å¦‚æœè¿”å›çš„å†…å®¹æ˜¯æƒ³è¦çš„ç­”æ¡ˆæ ·å¼å³ABCDä¸­çš„ä¸€ä¸ªå­—æ¯ï¼Œå°±å°†retryç½®å¦ï¼Œè¿™èƒ½ä½¿ç¨‹åºè·³å‡ºwhileå¾ªç¯è¿›å…¥ä¸‹ä¸€ä¸ªé—®é¢˜çš„å¤„ç†ã€‚
#             if formated_response:
#                 print("æ­¤ç­”æ¡ˆæ ¼å¼ç¬¦åˆé¢„æœŸï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªquestionçš„å¤„ç†")
#                 if llm_answer_compare(formated_response, item['answer']):
#                     correct += 1
#                 else:
#                     print("æ­¤é¢˜å¤§æ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆä¸æ­£ç¡®")
#                 total += 1
#                 retry = False
#             # å¦‚æœè¿”å›çš„ç­”æ¡ˆä¸æ˜¯æŒ‡å®šé€‰é¡¹ï¼Œä¿®æ”¹promptï¼Œå†è®©å¤§æ¨¡å‹å¤„ç†ä¸€éè¿™ä¸ªç»“æœï¼Œç›´åˆ°è¾“å‡ºçš„æ˜¯ä¸€ä¸ªå­—æ¯ä¸ºæ­¢
#             else:
#                 print("æ­¤ç­”æ¡ˆä¸æ ¼å¼ä¸ç¬¦åˆé¢„æœŸæ ¼å¼ï¼Œä¿®æ”¹prompté‡æ–°å†å¤„ç†ä¸€éå½“å‰question")
#                 prompt = prompt + "\n\n" + response + "the response is not a character represent one of the options, modify the result to be a character"
